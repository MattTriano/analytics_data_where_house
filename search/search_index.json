{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Analytics-Data-Where-House Docs!","text":""},{"location":"#table_of_contents","title":"Table of Contents","text":"<ol> <li>System setup</li> <li>User's Guide</li> <li>Developer's Guide</li> <li>Data Sources</li> </ol>"},{"location":"#platform_overview","title":"Platform Overview","text":"<p>This platform automates curating a local data warehouse of interesting, up-to-date public data sets. It enables users (well, mainly one user, me) to easily add data sets to the warehouse, build analyses that explore and answer questions with current data, and discover existing assets to accelerate exploring new questions.</p> <p>At present, it uses docker to provision and run:</p> <ul> <li>a PostgreSQL + PostGIS database as the data warehouse,</li> <li> <p>Apache Superset for:</p> <ul> <li>Interactive Data Visualization and EDA</li> <li>Dashboarding and Reporting</li> </ul> <p> </p> </li> <li> <p>a pgAdmin4 database administration interface,</p> <p></p> </li> <li> <p>Airflow components to orchestrate execution of tasks,</p> <p></p> </li> <li> <p>dbt to:</p> <ul> <li>manage sequential data transformation + cleaning tasks,</li> <li>serve data documentation and data lineage graphs, and</li> <li>facilitate search of the data dictionary and data catalog</li> </ul> <p> </p> </li> <li> <p>great_expectations for anomaly detection and data monitoring, and</p> <p></p> </li> <li> <p>custom python code that makes it easy to implement an ELT pipeline for any other table hosted by Socrata</p> <p></p> <p></p> <p></p> </li> </ul>"},{"location":"data_sources/socrata/","title":"Socrata ELT","text":""},{"location":"data_sources/socrata/#socrata_table_ingestion_flow","title":"Socrata Table Ingestion Flow","text":"<p>The Update-data DAGs for (at least) Socrata tables follow the pattern below: * Check the metadata of the table's data source (via api if available, or if not, by scraping where possible)   * If the local data warehouse's data is stale:     * download and ingest all new records into a temporary table,     * identify new records and updates to prior records, and     * add any new or updated records to a running table of all distinct records   * If the local data warehouse's data is as fresh as the source:     * update the freshness-check-metadata table and end</p> <p></p> <p>Before downloading potentially gigabytes of data, we check the data source's metadata to determine if the source data has been updated since the most recent successful update of that data in the local data warehouse. Whether there is new data or not, we'll log the results of that check in the data_warehouse's <code>metadata.table_metadata</code> table. </p> <p></p> <p></p> <p>If the data source's data is fresher than the data in the local data warehouse, the system downloads the entire table from the data source (to a file in the Airflow-scheduler container) and then runs the <code>load_data_tg</code> TaskGroup, which:</p> <ol> <li> <p>Loads it into a \"temp\" table (via the appropriate data-loader TaskGroup).</p> <p></p> </li> <li> <p>Creates a persisting table for this data set in the <code>data_raw</code> schema if the data set is a new addition to the warehouse.</p> </li> <li> <p>Checks if the initial dbt data_raw deduplication model exists, and if not, the <code>make_dbt_data_raw_model</code> task automatically generates a data-set-specific dbt data_raw model file.</p> <p></p> </li> <li> <p>Compares all records from the latest data set (in the \"temp\" table) against all records previously added to the persisting <code>data_raw</code> table. Records that are entirely new or are updates of prior records (i.e., at least one source column has a changed value) are appended to the persisting <code>data_raw</code> table.</p> <ul> <li>Note: updated records do not replace the prior records here. All distinct versions are kept so that it's possible to examine changes to a record over time.</li> </ul> </li> <li> <p>The <code>metadata.table_metadata</code> table is updated to indicate the table in the local data warehouse was successfully updated on this freshness check.</p> <p></p> <p>Those tasks make up the <code>load_data_tg</code> Task Group.</p> <p></p> </li> </ol> <p>If the local data warehouse has up-to-date data for a given data source, we will just record that finding in the metadata table and end the run.</p> <p></p>"},{"location":"data_sources/socrata/#data_loader_task_groups","title":"Data Loader task_groups","text":"<p>Tables with geospatial features/columns will be downloaded in the .geojson format (which has a much more flexible structure than .csv files), while tables without geospatial features (ie flat tabular data) will be downloaded as .csv files. Different code is needed to correctly and efficiently read and ingest these different formats. So far, this platform has implemented data-loader TaskGroups to handle .geojson and .csv file formats, but this pattern is easy to extend if other data sources only offer other file formats.</p> <p></p> <p>Many public data tables are exported from production systems, where records represent something that can change over time. For example, in this building permit table, each record represents an application for a building permit. Rather than adding a new record any time the application process moves forward (e.g., when a fee was paid, a contact was added, or the permit gets issued), the original record gets updated. After this data is updated, the prior state of the table is gone (or at least no longer publicly available). This is ideal for intended users of the production system (i.e., people involved in the process who have to look up the current status of a permit request). But for someone seeking to understand the process, keeping all distinct versions or states of a record makes it possible to see how a record evolved. So I've developed this workflow to keep the original record and all distinct updates for (non \"temp_\") tables in the <code>data_raw</code> schema.</p> <p>This query shows the count of new or updated records grouped by the data-publication DateTime when the record was new to the local data warehouse.</p> <p></p> <p>Fig: People really love using fireworks on New Years, and those alerts typically get updated after review.</p>"},{"location":"dev_guide/","title":"Using the system","text":""},{"location":"dev_guide/#adding_a_new_pipeline_of_a_socrata_data_asset","title":"Adding a new pipeline of a Socrata Data Asset","text":"<p>The Socrata platform is a wealth of public data and this system only requires a few manual steps to set up an ELT pipeline that adds a data set to your local warehouse.  </p>"},{"location":"dev_guide/#manual_steps","title":"Manual Steps","text":"<ol> <li> <p>Add a <code>SocrataTable</code> instance with the data set's table_id and table_name to the <code>/airflow/dags/sources/tables.py</code> file as shown here </p> </li> <li> <p>Copy this DAG into a file anywhere in <code>/airflow/dags/</code> and edit the 4 annotated lines.</p> </li> <li> <p>[In the Airflow Web UI] Run that new DAG.</p> </li> <li> <p>(Optional) To check data quality before updating your local table, set expectations for the data.</p> </li> <li> <p>(Optional) To standardize column names, dtypes, or order for a data set, edit the file named <code>{table_name}_standardized.sql</code> in directory <code>/airflow/dbt/models/standardized/</code>.</p> </li> </ol>"},{"location":"dev_guide/#workflow_overview","title":"Workflow Overview","text":"<p>The workflow for producing usable tables follows this pattern:</p> <ol> <li> <p>(<code>data_raw</code> schema): Set up an ingestion pipeline.</p> <p>1.1. Extract data to a local file.</p> <p>1.2. Load that data into a \"temp\" table.</p> <p>1.3. Select distinct records that aren't already in the warehouse and add them to a persistant table.</p> <p>1.4. Define a suite of expectations to validate future data updates.</p> </li> <li> <p>(<code>standardize</code> schema): Implement dbt models that standardize the data set's columns.</p> <p>2.1. Standardize column [names, dtypes, order] and perform cleaning steps in the <code>f\"{data_set_name}_standardized.sql</code> dbt model file.</p> </li> <li> <p>(<code>clean</code> schema): Automatically generates dbt models that implement a deduplication strategy, produce a clean data set.</p> </li> <li> <p>(<code>feature</code> schema): Implement dbt models to engineer data features.</p> <p>3.1. Engineer desired features</p> </li> <li> <p>(<code>dwh</code> schema): Implement dbt models to assemble data into analytically useful tables.</p> </li> </ol> <p>For tables hosted by Socrata, this system reduces steps 1.1 through 1.3 to a 3 minute operation, generates a nearly ready <code>..._standardized.sql</code> stub for 2.1, and automatically produces the <code>..._clean.sql</code> file from 2.2 after the <code>..._standardized.sql</code> stub is edited.</p> <p></p>"},{"location":"dev_guide/#census_data_sources","title":"Census Data Sources","text":""},{"location":"dev_guide/#adding_a_new_pipelines_for_a_census_api_data_set","title":"Adding a new pipelines for a Census API Data Set","text":"<p>Note</p> <p>This workflow is under active development and may change as new functionality is added.</p> <ol> <li> <p>Retrieve or refresh the catalog of Census API Datasets by running the <code>refresh_census_api_metadata</code> DAG.</p> </li> <li> <p>Identify a dataset of interest in the <code>metadata.census_api_metadata</code> table in the <code>re_dwh</code> database.</p> </li> <li> <p>Collect metadata on the dataset of interest's variables and geographies.</p> <p>3.1. Copy the <code>identifier</code> url for a dataset of interest.</p> <p>3.2. Paste that <code>identifier</code> url into the <code>DATASET_IDENTIFIER</code> variable near the top of the DAG file <code>/airflow/dags/refresh_census_api_dataset_variables_metadata.py</code></p> <p>3.3. Run the <code>refresh_census_api_dataset_variables_metadata</code> DAG</p> </li> <li> <p>To be developed</p> </li> </ol>"},{"location":"dev_guide/adding_a_socrata_pipeline/","title":"Adding a Socrata Pipeline","text":""},{"location":"dev_guide/adding_a_socrata_pipeline/#1_adding_a_new_pipeline_for_a_table_hosted_by_socrata","title":"1. Adding a new Pipeline for a table hosted by Socrata","text":"<p>After the system is set up, you can easily add a Socrata data set to the warehouse by</p> <p>Define the <code>SocrataTable</code> in <code>/airflow/dags/sources/tables.py</code>:</p> <p>Look up the table's documentation page on the web and get the <code>table_id</code> from the URL (it will be nine characters long, all lowercase and with a hyphen in the middle).</p> <pre><code>SAMPLE_DATA_SET = SocrataTable(\n    table_id=\"wvhk-k5uv\",             # (1)\n    table_name=\"sample_data_set\",   # (2)\n    schedule=\"0 6 4 * *\",             # (3)\n)\n</code></pre> <ol> <li>A Socrata table's <code>table_id</code> will always be 9 characters long and consist of two blocks of 4 characters (numbers or lowercase letters) separated by a hyphen. You can find the <code>table_id</code> in the data documentation URL or export link for the data set.</li> <li>Ideally the name of the <code>SocrataTable</code> instance should be the uppercased <code>table_name</code> (which should be lowercase).</li> <li>This sets the update frequency. If you aren't familiar with the <code>crontab</code> format, use cron expressions.</li> </ol> <p></p>"},{"location":"dev_guide/adding_a_socrata_pipeline/#2_create_a_dag_for_the_data_set","title":"2. Create a DAG for the data set","text":"<p>Copy this code into a new file in <code>/airflow/dags/</code> and edit the 4 annotated lines for the new data set:</p> <pre><code>import datetime as dt\nimport logging\n\nfrom airflow.decorators import dag\n\nfrom tasks.socrata_tasks import update_socrata_table\nfrom sources.tables import COOK_COUNTY_PARCEL_SALES as SOCRATA_TABLE   # (1)\n\ntask_logger = logging.getLogger(\"airflow.task\")\n\n\n@dag(\n    schedule=SOCRATA_TABLE.schedule,\n    start_date=dt.datetime(2022, 11, 1),\n    catchup=False,\n    tags=[\"cook_county\", \"parcels\", \"fact_table\", \"data_raw\"],        # (2)\n)\ndef update_data_raw_sample_data_set():                                # (3)\n    update_1 = update_socrata_table(\n        socrata_table=SOCRATA_TABLE,\n        conn_id=\"dwh_db_conn\",\n        task_logger=task_logger,\n    )\n    update_1\nupdate_data_raw_sample_data_set()                                     # (4)\n</code></pre> <ol> <li>Replace <code>COOK_COUNTY_PARCEL_SALES</code> with the name of the <code>SocrataTable</code> instance variable from <code>tables.py</code>.</li> <li>Change the tags to reflect this data set.</li> <li>Change the name of this DAG's function name to reflect this data set.</li> <li>Call that DAG function.</li> </ol>"},{"location":"dev_guide/making_expectation_suites/","title":"Data Validation with <code>great_expectations</code>","text":""},{"location":"dev_guide/making_expectation_suites/#configuring_a_checkpoint_and_validating_a_data_set","title":"Configuring a Checkpoint and Validating a Data Set","text":"<p>Note: If you've just generated your suite of expectations (ie if the notebook server is still up), shut down the notebook server without exiting the <code>py-utils</code> container. If things don't shut down nicely, enter <code>jupyter notebook stop 18888</code> to free up port 18888.</p> <p>In the <code>py-utils</code> container, you can generate a new checkpoint via <code>great_expectations checkpoint new &lt;some_descriptive_name&gt;</code>. Checkpoints can run one or more suite of expectations, so this project will name checkpoints via the convention <code>data_set_schema.data_set_table_name</code>. So for the expectation suite generated in the above section, command below will name the checkpoint and start up a jupyter server</p> <pre><code>root@c7cd3e337ddf:/home/great_expectations# great_expectations checkpoint new data_raw.cook_county_parcel_sales\nUsing v3 (Batch Request) API\nBecause you requested to create a new Checkpoint, we'll open a notebook for you now to edit it!\nIf you wish to avoid this you can add the `--no-jupyter` flag.\n\n\n[NotebookApp] Serving notebooks from local directory: /home/great_expectations/uncommitted\n[NotebookApp] Jupyter Notebook 6.5.2 is running at:\n[NotebookApp] http://&lt;container_id&gt;:18888/?token=&lt;a_long_token_string&gt;\n[NotebookApp]  or http://127.0.0.1:18888/?token=&lt;a_long_token_string&gt;\n[NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n</code></pre> <p>Open the notebook named <code>edit_checkpoint_{data_set_schema.data_set_table_name}.ipynb</code> and run the first code cell to run import statments and load the data context.</p> <p>The next data cell formats the checkpoint's configs. Look over the contents and confirm that it names the right table (data_asset), expectation suite, data source, etc. If anything looks off and you want to see the other valid options, run cells three and four. After making changes (I had to change both the <code>data_asset_name</code> and <code>expectation_suite_name</code>) the config for my SimpleCheckpoint looked like: </p> <pre><code>my_checkpoint_name = \"data_raw.cook_county_parcel_sales\" # This was populated from your CLI command.\n\nyaml_config = f\"\"\"\nname: {my_checkpoint_name}\nconfig_version: 1.0\nclass_name: SimpleCheckpoint\nrun_name_template: \"%Y%m%d-%H%M%S-my-run-name-template\"\nvalidations:\n  - batch_request:\n      datasource_name: where_house_source\n      data_connector_name: default_inferred_data_connector_name\n      data_asset_name: data_raw.cook_county_parcel_sales\n      data_connector_query:\n        index: -1\n    expectation_suite_name: data_raw.cook_county_parcel_sales.warning\n\"\"\"\n</code></pre> <p>When you're happy with the config, run that code cell to set the configs for your checkpoint and then run the Test your Checkpoint Configuration code cell to see if your config is valid. If it is (it will print out <code>... Successfully instantiated SimpleCheckpoint.</code>), the next cell allows you to review your Checkpoint and running the Add your Checkpoint cell will actually save the Checkpoint.</p> <p>If you want to run the validation checkpoint and generate data docs with the results of the checks, uncomment and run the last code cell. Those data docs can be found * on the host machine in:   * <code>.../airflow/great_expectations/uncommitted/data_docs/local_site/validations/data_raw/cook_county_parcel_sales/warning/&lt;%Y%m%d-%H%M%S&gt;-my-run-name-template/&lt;%Y%m%d-%H%M%S.%fZ&gt;/&lt;hash-looking-string&gt;.html</code> * In the jupyter notebook tree, at:   * <code>/data_docs/&lt;same_as_on_host_past__data_docs&gt;.html</code></p> <p>That data docs page will show you which expectations failed and allow you to review all of the expectations. You can manualy edit those expectations in the suite's <code>.json</code> file, or run <code>great_expectations suite edit data_raw.cook_county_parcel_sales.warning</code> (replace the suite as appropriate) at the command line in the <code>py-utils</code> container to interactively edit the suite.</p> <p>After reviewing your expectations editing or removing the unreasonable ones, you can rerun your checkpoint via a command like</p> <pre><code>great_expectations checkpoint run data_raw.cook_county_parcel_sales\n</code></pre>"},{"location":"dev_guide/standardizing_columns/","title":"Standardizing Column Names, dtypes, and order in the _standardized model","text":"<p>The <code>Update_raw_{data_set_name}</code> DAG will generate a dbt model file named <code>{data_set_name}_standardized.sql</code> in the dbt models directory <code>/airflow/dbt/models/standardized/</code>. This is where you should change column names, dtypes, and order.</p> <p>This file will have all of the data set's columns and their inferred dtypes, but you'll have to change at least the two REPLACE_WITH_... values before the model can be run. </p> <pre><code>{{ config(materialized='view') }}\n{% set ck_cols = [\"REPLACE_WITH_COMPOSITE_KEY_COLUMNS\"] %}\n{% set record_id = \"REPLACE_WITH_BETTER_id\" %}\nWITH records_with_basic_cleaning AS (\n    SELECT\n        {{ dbt_utils.generate_surrogate_key(ck_cols) }} AS {{ record_id }}\n        column_name_1::bigint AS column_name_1\n        ...\n</code></pre> <p>The <code>ck_cols</code> dbt variable should be a list of a set of column(s) that form a composite key that uniquely identifies a record across tables.</p> <p>If the table comes with a natural (single) primary key column, use that in the <code>ck_cols</code> list and delete both <code>record_id</code> lines.</p> <p>If it takes 2+ columns to uniquely identify a record, set <code>ck_cols</code> and <code>record_id</code> as shown (but leave the other <code>record_id</code> line alone). When choosing a string for the <code>record_id</code>, pick something that describes the unit of one table record (e.g. parcel_sale_id for parcel sales, or parcel_valuation_id for parcel assessments).</p> <pre><code>{% set ck_cols = [\"column_name_1\", \"column_name_2\", ..., \"column_name_n\"] %}\n{% set record_id = \"(something_descriptive)_id\" %}\n</code></pre>"},{"location":"dev_guide/standardizing_columns/#selecting_columns_that_uniquely_identify_a_record","title":"Selecting columns that uniquely identify a record","text":"<p>The pgAdmin4 interface makes it very easy to execute queries and explore tables; right click any database object and open up the Query tool. There, you can enter and run queries. Here are some useful ones.</p>"},{"location":"dev_guide/standardizing_columns/#query_cookbook","title":"Query Cookbook","text":""},{"location":"dev_guide/standardizing_columns/#see_a_few_rows","title":"See a few rows","text":"<p>To see 5 records from the table table_name in the schema_name schema, run query: </p> <pre><code>SELECT *\nFROM schema_name.table_name\nLIMIT 5\n</code></pre> <p>Subsequent examples will use reference actual schemas and tables.</p>"},{"location":"dev_guide/standardizing_columns/#number_of_records","title":"Number of records","text":"<p>This will return the number of records in the table.</p> <pre><code>SELECT COUNT(*)\nFROM data_raw.temp_chicago_traffic_crashes\n</code></pre>"},{"location":"dev_guide/standardizing_columns/#distinct_values_in_a_column","title":"Distinct values in a column","text":"<p>This will return the number of records with a distinct value in the named column (crash_record_id shown)</p> <pre><code>SELECT COUNT(DISTINCT crash_record_id)\nFROM data_raw.temp_chicago_traffic_crashes\n</code></pre> <p>If the table is large, <code>DISTINCT</code> is a bit slow. You can speed it up a little by changing to query:</p> <pre><code>SELECT COUNT(*)\nFROM (\n    SELECT DISTINCT crash_record_id\n    FROM data_raw.temp_chicago_traffic_crashes\n) AS temp\n</code></pre>"},{"location":"dev_guide/standardizing_columns/#confirming_a_set_of_columns_form_a_primarycomposite_key","title":"Confirming a set of column(s) form a primary/composite key","text":"<p>Add one or more columns to the <code>(partition by ...)</code> list and run the query. If the returned count is 0, then the column set is a valid composite key (or primary key if there's only one column). Aim to use the smallest set of columns that returns zero.</p> <pre><code>WITH composite_key_candidates AS (\n    SELECT row_number() over(partition by sale_document_num, pin) as rn\n    FROM data_raw.cook_county_parcel_sales\n)\n\nSELECT count(*)\nFROM composite_key_candidates\nWHERE rn &gt; 1\n</code></pre> <p>The <code>row_number() over(partition by col1, col2, ..., coln) as rn...</code> operation will partition the data into groups with every distinct combination of values in the named columns, and within each grouping, it will produce row numnbers (from 1 up to the number of records in that group). </p> <p>If the set of columns produces a set of values that can identify every distinct record, there shouldn't be any records where the row number (<code>rn</code>) is above 1.</p>"},{"location":"dev_guide/standardizing_columns/#records_with_duplicated_values_in_a_given_column","title":"Records with duplicated values in a given column","text":"<p>When identifying a set of distinct columns, often one column (crash_record_id in this example) is highly unique by itself, but not perfectly unique. This query shows you all columns for the set of records with duplicated crash_record_id values. Looking through column values, look for the other features that changed. </p> <pre><code>WITH distinct_colpair_count AS (\n    SELECT crash_record_id, \n           row_number() over(partition by crash_record_id) as rn\n    FROM data_raw.temp_chicago_traffic_crashes\n),\ndupe_crash_ids AS (\n    SELECT *\n    FROM data_raw.chicago_traffic_crashes\n    WHERE crash_record_id IN (\n        SELECT crash_record_id\n        FROM distinct_colpair_count\n        WHERE rn &gt; 1\n    )\n    ORDER BY crash_record_id\n)\n\nSELECT *\nFROM dupe_crash_ids\n</code></pre> <p>To see the counts of each value in a column (<code>work_zone_type</code> shown) <pre><code>SELECT count(*) AS row_count, work_zone_type \nFROM data_raw.chicago_traffic_crashes\nGROUP BY work_zone_type\nORDER BY row_count\n</code></pre></p> <p>Throw in a <code>length()</code> feature if you want to see string lengths (useful if you want to specify the number of chars for a column)</p> <pre><code>SELECT count(*) AS row_count, length(work_zone_type) AS n_chars, work_zone_type \nFROM data_raw.chicago_traffic_crashes\nGROUP BY work_zone_type\nORDER BY row_count\n</code></pre>"},{"location":"dev_guide/standardizing_columns/#extracting_date_or_time_components_from_timestamp-valued_columns","title":"Extracting date or time components from timestamp-valued columns","text":"<p>This is often useful when trying to evaluate whether a time-zone correction is needed or was incorrectly applied. From domain knowledge, I know that traffic is worst from 8am to 10am (when many are driving to work) and from 4pm to 6pm (when many are driving home). I reason that the rate of crashes will be proportional to the number of vehicles on the road, so the distribution of crash-counts-by-hour should show peaks around both rush hours and a minimum from ~4am to 6am (log after bars close but before most drivers leave for work). If the peaks and valley line up with the expected times, I know the data was published in Chicago time (which is UTC-5 or UTC-6 depending on daylight savings time). If the distribution showed a minimum around 10am and peaks 1pm and 10pm, I'd know the data was published as UTC-0.</p> <pre><code>WITH date_col AS (\n    SELECT crash_date::timestamp AS crash_date\n    FROM data_raw.chicago_traffic_crashes\n)\n\nSELECT count(*), extract(hour FROM crash_date) AS date_hour\nFROM date_col\nGROUP BY date_hour\nORDER BY date_hour\n</code></pre> <p>Check the documentation to see what else <code>extract</code> can extract.</p>"},{"location":"dev_guide/standardizing_columns/#casting_4-digit_years_to_datelike_values","title":"Casting 4-digit years to datelike values","text":"<pre><code>to_date(tax_year::varchar, 'yyyy') AS tax_year\n</code></pre>"},{"location":"dev_guide/standardizing_columns/#zero_or_left-padding_strings_incorrectly_parsed_to_a_numeric_type","title":"Zero or left-padding strings incorrectly parsed to a numeric type","text":"<p>If you know that the column values should all be strings of the same length but the leading values were ignored (often zeros or spaces), cast the column (or feature) to a text/char/varchar type and left-pad it with a padding character to a given length (shown using <code>0</code> as the char and 2 as the length).</p> <pre><code>SELECT lpad(extract(month FROM crash_date::timestamp)::char(2), 2, '0') AS crash_month\nFROM data_raw.chicago_traffic_crashes\n</code></pre>"},{"location":"dev_guide/standardizing_columns/#removing_punctuation_or_numbers_from_strings","title":"Removing punctuation or numbers from strings","text":"<p>If you want to replace any characters except for letters or spaces, use <code>regexp_replace()</code> with regex pattern <code>[^A-Z ]</code> (<code>^</code> means \"except\", and the <code>g</code> flag indicates \"replace all matches\").</p> <pre><code>WITH alpha_only AS (\n    SELECT regexp_replace(upper(city::text), '[^A-Z ]', '', 'g') as city\n    FROM data_raw.chicago_food_inspections\n)\n\nSELECT count(*), city\nFROM alpha_only\nGROUP BY city\nORDER BY count desc\n</code></pre>"},{"location":"dev_guide/standardizing_columns/#type_casting","title":"Type Casting","text":"<p>Useful casting</p> <ul> <li><code>&lt;col_name&gt;::smallint</code> Small Integers (up to 2^16, or 65536)</li> <li><code>&lt;col_name&gt;::int</code> Small Integers (up to 2^32, or 4294967296)</li> <li><code>&lt;col_name&gt;::bigint</code> Small Integers (up to 2^64, or 1.8446744e+19)</li> <li><code>&lt;col_name&gt;::double precision</code> Floats (decimal numerics)</li> <li><code>&lt;col_name&gt;::real::bigint</code> Useful for trimming trailing decimal zeros from integer-valued numbers</li> </ul>"},{"location":"dev_guide/standardizing_columns/#references","title":"References:","text":"<ul> <li>Review issue 55 if you're interested in the thought process for the <code>_standardized</code> stage.</li> </ul>"},{"location":"dev_guide/feature_engineering/","title":"Engineering Features","text":""},{"location":"dev_guide/feature_engineering/#engineering_geospatial_features","title":"Engineering Geospatial Features","text":""},{"location":"dev_guide/feature_engineering/#engineering_time_series_features","title":"Engineering Time Series Features","text":""},{"location":"dev_guide/feature_engineering/geospatial/","title":"Engineering Geospatial Features","text":"<p>PostGIS is a PostgreSQL database extension that adds support for geographic objects and provides extensive functionality for representing and analyzing spatial data.</p>"},{"location":"dev_guide/feature_engineering/geospatial/#engineering_point_locations_from_latitude_and_longitude_values","title":"Engineering Point Locations from Latitude and Longitude values","text":"<p>If you have <code>Latitude</code> and <code>Longitude</code> values (and know the Spatial Reference ID, aka SRID, of the system that produced those values), you can produce a geometric <code>Point</code> representing the location for each <code>(Latitude, Longitude)</code> pair.</p> <pre><code>SELECT\n    parcel_location_id,\n    pin,\n    ST_SetSRID(ST_MakePoint(longitude, latitude), 4326) AS geometry\nFROM clean.cook_county_parcel_locations_clean\n</code></pre>"},{"location":"dev_guide/feature_engineering/geospatial/#engineering_boundaries_from_point_locations","title":"Engineering Boundaries from Point Locations","text":"<p>There are a number of algorithms for producing polygons from sets of points.</p>"},{"location":"dev_guide/feature_engineering/geospatial/#convex_hull","title":"Convex Hull","text":"<p>The Convex Hull algorithm produces a polygon by connecting the outermost points in a group. The <code>ConvexHull</code> algorithm is computationally inexpensive, but it will typically produce overlapping polygons.</p> <p>As a mental heuristic, this algorithm produces a polygon by looping a string around all of the points and pulling tight. </p> <pre><code>WITH school_elem_locs AS (\n    SELECT\n        school_elem_district,\n        ST_SetSRID(ST_MakePoint(longitude, latitude), 4326) AS geometry\n    FROM clean.cook_county_parcel_locations_clean\n)\n\nSELECT\n    school_elem_district,\n    ST_ConvexHull(ST_Collect(geometry)) AS school_elem_district_boundary\nFROM school_elem_locs\nGROUP BY school_elem_district\n</code></pre> <p></p>"},{"location":"dev_guide/feature_engineering/geospatial/#concave_hull","title":"Concave Hull","text":"<p>The Concave Hull algorithm is much (MUCH) more computationally intensive and typically requires some experimentation to dial in the parameters, but it can produce much more complex polygons. The <code>param_pctconvex</code> parameter accepts values from 0 to 1, and the lower the value, the shorter the gap between perimeter points before the edge will erode (and the longer the calculation will take).</p> <pre><code>SELECT\n    school_elem_district,\n    ST_ConcaveHull(\n        param_geom =&gt; ST_Collect(geometry),\n        param_pctconvex =&gt; 0.25,\n         param_allow_holes =&gt; false\n    ) AS school_elem_district_boundary\nFROM school_elem_locs\nGROUP BY school_elem_district\n</code></pre> <p>Warning: This algorithm is exceptionally slow over large sets of points. If you absolutely need to use this algorithm, minimize the number of points in the calculation, and start with a higher <code>param_pctconvex</code> value and reduce it if needed. For reference, running this query over ~78k points with <code>param_pctconvex</code> set to 0.3 took just over 2.5 minutes to finish, but when set to 0.1, it hadn't finished after 38 minutes. </p> <p> </p>"},{"location":"dev_guide/feature_engineering/geospatial/#other_polygon-producing_algorithms","title":"Other Polygon-producing Algorithms","text":"<p>There are a few other noteable algorithms for engineering polygons from points, namely variants of Alpha Shape, but they require the SFCGAL PostgreSQL extension which isn't currently included in the PostGIS database container (although adding it would be fairly easy for new ADWH instances).</p> <p>Alpha Shape Reference</p>"},{"location":"dev_guide/feature_engineering/time_series/","title":"Engineering Temporal (Time Series) Features","text":"<pre><code>SELECT\n    to_char(sale_date, 'YYYY-MM')       AS month_of_sale,\n    count(sale_price)                   AS num_sales,\n    max(sale_price)                     AS max_sale_price,\n    avg(sale_price)::bigint             AS mean_sale_price,\n    min(sale_price)                     AS min_sale_price,\n    percentile_cont(0.5) WITHIN GROUP (order by sale_price) AS median_sale_price    \nFROM feature.cook_county_parcel_price_change_between_sales\nGROUP BY month_of_sale\nORDER BY month_of_sale\n</code></pre>"},{"location":"dev_guide/feature_engineering/time_series/#generating_an_evenly_spaced_series_of_dates","title":"Generating an evenly spaced series of dates","text":"<pre><code>WITH sale_counts AS (\n    SELECT\n        count(*),\n        to_char(sale_date, 'YYYY-MM')       AS month_of_sale\n    FROM dwh.cook_county_parcel_sales_fact\n    GROUP BY month_of_sale\n),\nend_dates AS (\n    SELECT *\n    FROM generate_series(\n        (SELECT (min(month_of_sale) || '-01')::date FROM sale_counts WHERE count &gt;= 5),\n        (SELECT (max(month_of_sale) || '-01')::date FROM sale_counts WHERE count &gt;= 5),\n        interval '1 month'\n    )       \n)\n\nSELECT * FROM end_dates\n</code></pre> <p>Syntax decoder: <code>||</code> is the same as <code>concat()</code></p>"},{"location":"dev_guide/validation/","title":"Data Validation with <code>great_expectations</code>","text":"<p><code>great_expectations</code> is a tool that can help you ensure the quality and integrity of your data. It allows you to define expectations for your data and automatically check that those expectations are met. This can help you catch and fix any issues before they lead to problems downstream.</p> <p>Here is a sample workflow for setting expectations for tables in the data warehouse.</p>"},{"location":"dev_guide/validation/data_assistant_workflow/","title":"Data assistant workflow","text":""},{"location":"dev_guide/validation/data_assistant_workflow/#generating_a_suite_of_expectations_via_the_data_assistant","title":"Generating a Suite of Expectations via the Data Assistant","text":"<p>To use <code>great_expectations</code>'s Data Assistant to generate a suite of expectations for a data set interactively, first start the <code>py-utils</code> service's container and <code>cd</code> into the <code>great_expectations/</code> directory</p> <pre><code>make get_py_utils_shell\n...\nroot@&lt;container_id&gt;:/home# cd great_expectations/\n</code></pre> <p>Then enter this to bring up suite-generation prompts</p> <pre><code>root@&lt;container_id&gt;:/home/great_expectations# great_expectations suite new\n</code></pre> <p>At this prompt, enter 3 to use the Data Assistant to automatically generate some expectations (after you specify which columns to ignore in a notebook)</p> <pre><code>How would you like to create your Expectation Suite?\n    1. Manually, without interacting with a sample Batch of data (default)\n    2. Interactively, with a sample Batch of data\n    3. Automatically, using a Data Assistant\n: 3\n</code></pre> <p>Then select the data set to set expectations for</p> <pre><code>Which data asset (accessible by data connector \"default_inferred_data_connector_name\") would you like to use?\n...\n    10. data_raw.cook_county_parcel_locations\n    11. data_raw.cook_county_parcel_sales\n    12. data_raw.cook_county_parcel_value_assessments\n...\nType [n] to see the next page or [p] for the previous. When you're ready to select an asset, enter the index.\n: 11\n</code></pre> <p>and use the default name by pressing enter and entering <code>y</code> when asked</p> <pre><code>Name the new Expectation Suite [data_raw.cook_county_parcel_sales.warning]:\n\nGreat Expectations will create a notebook, containing code cells that select from available columns in your dataset and\ngenerate expectations about them to demonstrate some examples of assertions you can make about your data.\n\nWhen you run this notebook, Great Expectations will store these expectations in a new Expectation Suite \"data_raw.cook_county_parcel_sales.warning\" here:\n\n  file:///home/great_expectations/expectations/data_raw/cook_county_parcel_sales/warning.json\n\nWould you like to proceed? [Y/n]: y\n</code></pre> <p>Now that a data set is selected, <code>great_expectations</code> will generate a notebook for your suite and spin up a jupyter server on port 18888 (which is mapped to port 18888 on the host system). In a browser, go to either of URLs in the output and open the notebook named <code>edit_{the default name of the suite from the last step}.ipynb</code>.</p> <pre><code>Opening a notebook for you now to edit your expectation suite!\nIf you wish to avoid this you can add the `--no-jupyter` flag.\n\n\n[NotebookApp] Serving notebooks from local directory: /home/great_expectations/uncommitted\n[NotebookApp] Jupyter Notebook 6.5.2 is running at:\n[NotebookApp] http://&lt;container_id&gt;:18888/?token=&lt;a_long_token_string&gt;\n[NotebookApp]  or http://127.0.0.1:18888/?token=&lt;a_long_token_string&gt;\n[NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n</code></pre> <p>In the notebook, run the first code cell without changes (maybe increase the batch size if you want better initial expectations, although it will take longer to generate those initial expectations).</p> <p>In the second code cell, you'll indicate the columns to exclude from the automatic expectation generation process. I find it's easier to just comment out column names and run the sell so that the <code>exclude_column_names</code> variable is defined (and set equal to an empty list).</p> <p>Then run the remaining two code cells. The third code cell determines values for each of the initial expectations for each columns, and the fourth cell formats the expectations into <code>json</code> and writes them to a file in the location indicated in the suite naming step.</p> <p>At this point, you can exit out of the notebook and delete it if you want. These expectations are intentionally not production-ready and some will fail if/when you try to use them to validate the full data set, so you'll have to review and edit these expectations while configuring a Checkpoint.</p> <p>Those expectations are in the <code>.json</code> file in a subdirectory of the <code>./airflow/great_expectations/expectations/</code> directory, and the relative path will be the name given to the expectation suite (ie <code>.../expectations/data_raw/cook_county_parcel_sales/warning.json</code>).</p>"},{"location":"dev_guide/validation/data_source_setup/","title":"Setting up New Data Sources","text":"<p>This project already configures a <code>great_expectations</code> Datasource and Data Connectors for the included <code>dwh_db</code> database, but if you want to set up another Datasource (ie a connection to another data source), you can interactively set up and test a configuration via the following steps:</p> <p>start the <code>py-utils</code> service's container and <code>cd</code> into the <code>great_expectations/</code> directory</p> <pre><code>make get_py_utils_shell\n...\nroot@&lt;container_id&gt;:/home# cd great_expectations/\n</code></pre> <p>Then enter this to bring up Datasource-configuration prompts and enter values as appropriate. The example below shows steps for configuring another PostgreSQL Datasource.</p> <pre><code>root@&lt;container_id&gt;:/home/great_expectations# great_expectations datasource new\nUsing v3 (Batch Request) API\n\nWhat data would you like Great Expectations to connect to?\n    1. Files on a filesystem (for processing with Pandas or Spark)\n    2. Relational database (SQL)\n: 2\n\nWhich database backend are you using?\n    1. MySQL\n    2. Postgres\n    3. Redshift\n    4. Snowflake\n    5. BigQuery\n    6. Trino\n    7. other - Do you have a working SQLAlchemy connection string?\n: 2\n\nBecause you requested to create a new Datasource, we'll open a notebook for you now to complete it!\n[NotebookApp] Serving notebooks from local directory: /home/great_expectations/uncommitted\n[NotebookApp] Jupyter Notebook 6.5.2 is running at:\n[NotebookApp] http://&lt;container_id&gt;:18888/?token=&lt;a_long_token_string&gt;\n[NotebookApp]  or http://127.0.0.1:18888/?token=&lt;a_long_token_string&gt;\n</code></pre> <p>Go to either of the jupyter URLs shown and open the just-created notebook file named something similar to <code>datasource_new.ipynb</code>. Edit cells as as appropriate following the instructions in the notebook.</p> <p>Run the Test Your Datasource Configuration cell to test the configuration. You might have to enter plaintext credentials in this notebook and then replace the plaintext strings with the name of the appropriate environment variable after writing the configuration to the <code>great_expectations.yml</code> file (eg for the <code>password:</code> field, replace the actual password with <code>${SOURCE_PASSWORD_NAME_IN_.env_file}</code>).</p> <p>After testing indicates the connection works, run the last cell to add the configuration to the <code>great_expectations.yml</code> config file in <code>/airflow/great_expectations/</code>. Note: Replace any plaintext credential strings with variables before committing the file to source control.</p>"},{"location":"dev_guide/validation/manually_setting_expectations/","title":"Developing a Suite of Expectations Manually","text":"<p>To manually develop your suite of expectations, get a shell in the <code>py-utils</code> container and <code>cd</code> into the <code>great_expectations/</code> directory</p> <pre><code>user@host:~/...$ make get_py_utils_shell \ndocker compose up -d py-utils\ncc_real_estate_dbt_airflow_ge_py-utils_1 is up-to-date\ndocker compose exec py-utils /bin/bash\nroot@b5a3b6c2727f:/home# cd great_expectations/\n</code></pre> <p>Enter <code>great_expectations suite new</code> to start making a new suite of expectations and enter 1 to select the manual workflow. </p> <pre><code>root@b5a3b6c2727f:/home/great_expectations# great_expectations suite new\nUsing v3 (Batch Request) API\n\nHow would you like to create your Expectation Suite?\n    1. Manually, without interacting with a sample Batch of data (default)\n    2. Interactively, with a sample Batch of data\n    3. Automatically, using a Data Assistant\n: 1\n</code></pre> <p>When prompted for a name for your suite of expectations, use this format</p> <pre><code>suite_name = f\"{schema_name}.{table_name}.warning\"\n</code></pre> <p>So if the <code>table_name</code> is temp_chicago_shotspotter_alerts and the table is in the data_raw schema, the <code>suite_name</code> will be data_raw.temp_chicago_shotspotter_alerts.warning and output the expectation suite to the file <code>.../expectations/data_raw/temp_chicago_shotspotter_alerts/warning.json</code>.</p> <p>When prompted, enter that suite name. <code>great_expectations</code> will initialize the <code>.json</code> file and an <code>.ipynb</code> notebook file for this suite, and then spin up a jupyter lab server so you can add expectations via that notebook file.</p> <pre><code>Name the new Expectation Suite [warning]: data_raw.temp_chicago_shotspotter_alerts.warning\nOpening a notebook for you now to edit your expectation suite!\nIf you wish to avoid this you can add the `--no-jupyter` flag.\n\n[I ServerApp] jupyter_server_terminals | extension was successfully linked.\n[I ServerApp] jupyterlab | extension was successfully linked.\n...\n...\n[I ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n[W ServerApp] No web browser found: could not locate runnable browser.\n[C ServerApp] \n\n    To access the server, open this file in a browser:\n        file:///root/.local/share/jupyter/runtime/jpserver-290-open.html\n    Or copy and paste one of these URLs:\n        http://b5a3b6c2727f:18888/lab?token=ebd251c23bd05cbdba8240305fc469a1314955ffb1438802\n     or http://127.0.0.1:18888/lab?token=ebd251c23bd05cbdba8240305fc469a1314955ffb1438802\n</code></pre> <p>Go to either of those URLs and open the <code>edit_&lt;suite_name&gt;.ipynb</code> notebook. </p> <p>The first cell will:</p> <ol> <li>import necessary importables (not shown),</li> <li>load this project's DataContext,</li> <li>set the <code>expectation_suite_name</code> variable, and</li> <li>check if there's expectation suite file with that name:<ul> <li>if <code>True</code>: load that expectation suite into the <code>suite</code> variable.</li> <li>if <code>False</code>: create a new and empty suite of expectations with that name.</li> </ul> </li> </ol> <pre><code>context = ge.data_context.DataContext()\n# Feel free to change the name of your suite here.\n#   Renaming this will not remove the other one.\nexpectation_suite_name = \"data_raw.temp_chicago_shotspotter_alerts.warning\"\ntry:\n    suite = context.get_expectation_suite(\n        expectation_suite_name=expectation_suite_name\n    )\n    print(\n        f'Loaded ExpectationSuite \"{suite.expectation_suite_name}\" ' +\n        f'containing {len(suite.expectations)} expectations.'\n    )\nexcept DataContextError:\n    suite = context.create_expectation_suite(\n        expectation_suite_name=expectation_suite_name\n    )\n    print(f'Created ExpectationSuite \"{suite.expectation_suite_name}\".')\n</code></pre>"},{"location":"dev_guide/validation/manually_setting_expectations/#creating_expectations","title":"Creating Expectations","text":"<p>Now that you have an <code>ExpectationSuite</code> object, you can start defining and adding specific <code>Expectations</code>. The generated notebook provides headings for Table Expectation(s) and Column Expectation(s) where you can configure <code>Expectations</code> from the <code>Expectation</code> Gallery to reflect your expectations for the data table or specific columns.</p> <p>All <code>Expectations</code> will have at least these three optional arguments:</p> <ul> <li><code>result_format</code>: A string or dict specifying which fields to return when the <code>Expectation</code> is evaluated.</li> <li><code>catch_exceptions</code>: A boolean that determines how <code>great_expectations</code> shouve when an error or exception is raised during  evaluation of the <code>Expectation</code>.</li> <li><code>meta</code>: A dict of user-supplied metadata to be stored with the <code>Expectation</code>.</li> </ul> <p><code>ColumnMapExpectation</code>s and <code>MultipleColumnMapExpectation</code>s also have another standard (optional) argument, <code>mostly</code>.</p> <ul> <li><code>mostly</code>: A float between 0 and 1 (inclusive) indicating what proportion of column values must meet the <code>Expectation</code>.</li> </ul>"},{"location":"dev_guide/validation/manually_setting_expectations/#table_expectations","title":"Table Expectations","text":"<p><code>TableExpectations</code> are, unsurprisingly, expectations for table-level properties. At the time of writing, there are eight core <code>TableExpectations</code>. I set these <code>TableExpectations</code> in nearly every <code>ExpectationSuite</code> I develop:</p> <ul> <li><code>expect_table_row_count_to_be_between</code>: Expect the number of rows to be between two values.</li> <li><code>expect_table_columns_to_match_set</code>: Expect the columns to match an unordered set.</li> </ul> <p>On occasion, a few of the remaining six may be worth adding.</p> <ul> <li><code>expect_table_columns_to_match_ordered_list</code>: Expect the columns to exactly match a specified list.</li> <li><code>expect_column_to_exist</code>: Expect the specified column to exist.</li> <li><code>expect_table_row_count_to_equal_other_table</code>: Expect the number of rows to equal the number in another table.</li> <li><code>expect_table_column_count_to_be_between</code>: Expect the number of columns to be between two values.</li> <li><code>expect_table_column_count_to_equal</code>: Expect the number of columns to equal a value.</li> <li><code>expect_table_row_count_to_equal</code>: Expect the number of rows to equal a value.</li> </ul>"},{"location":"dev_guide/validation/manually_setting_expectations/#defining_a_tableexpectation","title":"Defining a <code>TableExpectation</code>","text":"<p>In the notebook, you can define a <code>TableExpectation</code> by creating an <code>ExpectationConfiguration</code> instance with parameters:</p> <ul> <li><code>expectation_type</code>: the name of the <code>TableExpectation</code>,</li> <li><code>kwargs</code>: a <code>dict</code> of keyword arguments that enable you to represent your expectations for data values (the available kwargs can be found in the <code>Expectation</code>'s documentation from the Gallery).</li> </ul> <pre><code># Create an Expectation\nexpectation_configuration = ExpectationConfiguration(\n    # Name of expectation type being added\n    # expectation_type=\"expect_table_columns_to_match_set\",\n    # These are the arguments of the expectation\n    # The keys allowed in the dictionary are Parameters and\n    # Keyword Arguments of this Expectation Type\n    kwargs={\n        \"column_list\": [\n            \"account_id\", \"user_id\", \"transaction_id\",\n            \"transaction_type\", \"transaction_amt_usd\"\n        ]\n    },\n    # This is how you can optionally add a comment about this expectation.\n    # It will be rendered in Data Docs.\n    # See this guide for details:\n    # `How to add comments to Expectations and display them in Data Docs`.\n    meta={\n        \"notes\": {\n            \"format\": \"markdown\",\n            \"content\": \"\"\"\n                Some clever comment about this expectation.\n                **Markdown** `Supported`\n            \"\"\"\n        }\n    }\n)\n</code></pre> <p>Then add that expectation to your <code>suite</code></p> <pre><code># Add the Expectation to the suite\nsuite.add_expectation(expectation_configuration=expectation_configuration)\n</code></pre>"},{"location":"dev_guide/validation/manually_setting_expectations/#column_expectations","title":"Column Expectations","text":"<p>There are far more <code>Column</code>-based <code>Expectations</code> (especially if you include user-contributed <code>Expectations</code> in the count). Here are some of the most generally applicable ones:</p>"},{"location":"dev_guide/validation/manually_setting_expectations/#for_categorically-valued_columns","title":"For Categorically-valued columns:","text":"<ul> <li><code>expect_column_distinct_values_to_be_in_set</code></li> <li><code>expect_column_distinct_values_to_equal_set</code></li> </ul>"},{"location":"dev_guide/validation/manually_setting_expectations/#for_numerically-valued_columns","title":"For Numerically-valued columns:","text":"<ul> <li><code>expect_column_mean_to_be_between</code></li> <li><code>expect_column_median_to_be_between</code></li> <li><code>expect_column_most_common_value_to_be_in_set</code></li> </ul>"},{"location":"dev_guide/validation/manually_setting_expectations/#for_string-valued_columns","title":"For String-valued columns:","text":"<ul> <li><code>expect_column_value_lengths_to_be_between</code></li> <li><code>expect_column_values_to_match_regex</code></li> </ul>"},{"location":"dev_guide/validation/manually_setting_expectations/#for_datetime-valued_columns","title":"For Date[Time]-valued columns:","text":"<ul> <li><code>expect_column_values_to_be_dateutil_parseable</code>: Note: Only implemented for pandas DataFrames.</li> <li><code>expect_column_values_to_match_strftime_format</code>: Note: Only implemented for pandas and spark DataFrames.</li> </ul>"},{"location":"dev_guide/validation/manually_setting_expectations/#regarding_columns_properties","title":"Regarding columns properties:","text":"<ul> <li><code>expect_column_values_to_be_unique</code></li> <li><code>expect_column_values_to_be_of_type</code></li> <li><code>expect_column_values_to_not_be_null</code></li> </ul> <p>Check the <code>Expectation</code> Gallery to review all available features.</p>"},{"location":"dev_guide/validation/manually_setting_expectations/#defining_a_columnexpectation","title":"Defining a <code>ColumnExpectation</code>","text":"<p>As with <code>TableExpectations</code>, setting <code>ColumnExpectations</code> for a column will </p> <pre><code>expectation_configuration = ExpectationConfiguration(\n    expectation_type=\"expect_column_value_lengths_to_be_between\",\n    kwargs={\n        \"column\": \"zip_code\",\n        \"min_value\": 5,\n        \"max_value\": 5,\n    }\n)\nsuite.add_expectation(expectation_configuration=expectation_configuration)\n</code></pre>"},{"location":"dev_guide/validation/manually_setting_expectations/#saving_and_reviewing_expectations","title":"Saving and Reviewing Expectations","text":"<p>After setting expectations, run the last cell to</p> <ol> <li>Print out the expectations in your <code>expectation_suite</code>,</li> <li>save your <code>expectation_suite</code> to file,</li> <li>generate data docs for your <code>expectation_suite</code>, and </li> <li>unsuccessfully attempt to open those data docs.<ul> <li>You should be able to find them in the location <code>/airflow/great_expectations/uncommitted/data_docs/local_site/expectations/&lt;your_expectation_suite's_name&gt;.html</code>.</li> </ul> </li> </ol> <pre><code>print(\n    context.get_expectation_suite(expectation_suite_name=expectation_suite_name)\n)\ncontext.save_expectation_suite(\n    expectation_suite=suite,\n    expectation_suite_name=expectation_suite_name\n)\n\nsuite_identifier = ExpectationSuiteIdentifier(\n    expectation_suite_name=expectation_suite_name\n)\ncontext.build_data_docs(resource_identifiers=[suite_identifier])\ncontext.open_data_docs(resource_identifier=suite_identifier)\n</code></pre> <p>At this point, you're ready to set up a <code>Checkpoint</code> for your suite.</p>"},{"location":"dev_guide/validation/manually_setting_expectations/#resources","title":"Resources","text":"<ul> <li>Official doc on manual expectation setting.</li> <li>The Expectation Gallery</li> </ul>"},{"location":"dev_guide/validation/new_workflow/","title":"Great Expectations Workflow","text":""},{"location":"dev_guide/validation/new_workflow/#starting_up_the_jupyter_server","title":"Starting up the Jupyter Server","text":"<p>Run the <code>make</code> command </p> <p><code>$ make serve_great_expectations_jupyterlab</code></p> <p>to start up the jupyter lab server where you can create and edit expectations and checkpoints. You will see output like what is shown below. </p> <pre><code>$ make serve_great_expectations_jupyterlab \ndocker compose exec airflow-scheduler /bin/bash -c \\\n        \"mkdir -p /opt/airflow/.jupyter/share/jupyter/runtime &amp;&amp;\\\n        cd /opt/airflow/great_expectations/ &amp;&amp;\\\n        jupyter lab --ip 0.0.0.0 --port 18888\"\n[I ServerApp] Package jupyterlab took 0.0000s to import\n...\n[I ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n[W ServerApp] No web browser found: Error('could not locate runnable browser').\n[C ServerApp] \n\n    To access the server, open this file in a browser:\n        file:///opt/airflow/.jupyter/share/jupyter/runtime/jpserver-164217-open.html\n    Or copy and paste one of these URLs:\n        http://44a94924c5da:18888/lab?token=be72207c3a182bed5c026af4c3014765250e98e0f8994d9c\n        http://127.0.0.1:18888/lab?token=be72207c3a182bed5c026af4c3014765250e98e0f8994d9c\n</code></pre> <p>Copy the URL starting with <code>http://127.0.0.1:18888/...</code> and paste it into a browser.</p> <p>Note</p> <p>To copy the URL, highlight it, right click it, and select Copy. Don't press ctrl+c as that will shut down the jupyter server.</p> <p>This will bring you to a jupyterlab interface where you can develop <code>GX</code> resources in the execution environment.</p> <p>There are several Expectation development workflows. This demo shows the interactive workflow.</p>"},{"location":"dev_guide/validation/new_workflow/#creating_expectations_interactively","title":"Creating Expectations Interactively","text":"<p>In the file browser on the left, go into the <code>uncommitted</code> directory and create a new notebook.</p> <p>Load the <code>DataContext</code> and our data_warehouse <code>Datasource</code></p> <pre><code>import great_expectations as gx\n\ncontext = gx.get_context()\ndatasource = context.get_datasource(datasource_name=\"fluent_dwh_source\")\n</code></pre> <p>Pick and load a <code>DataAsset</code> to develop <code>Expectations</code> for. You can view all registered <code>DataAssets</code> via <code>datasource.get_asset_names()</code>.</p> <pre><code>data_asset_name = f\"data_raw.temp_chicago_sidewalk_cafe_permits\"\nexpectation_suite_name = f\"{data_asset_name}_suite\"\ndata_asset = datasource.get_asset(asset_name=data_asset_name)\n</code></pre> <p>[Optional]: Define a strategy for splitting the data into batches and sorting those data batches. In this example, I've split the data by year and month based on a <code>date</code>-valued column in the dataset.</p> <pre><code>data_asset.add_splitter_year_and_month(column_name=\"issued_date\")\ndata_asset.add_sorters([\"+year\", \"+month\"])\n</code></pre> <p>Get a <code>Validator</code> instance for the current <code>DataAsset</code> and inspect a few rows of a batch.</p> <pre><code>validator = context.get_validator(\n    batch_request=data_asset.build_batch_request(),\n    expectation_suite_name=expectation_suite_name,\n)\nvalidator.head()\n# Or, if you want to explore a full batch\n# sample_df = validator.head(fetch_all=True)\n</code></pre> How-to: View all <code>DataFrame</code> columns <p>If your dataset has more than 20 columns (<code>pandas</code>' default <code>max_columns</code> value), <code>validator.head()</code> will show a truncated view of the <code>DataFrame</code>. To view all columns, change the <code>max_columns</code> setting for your notebook by running this:</p> <pre><code>import pandas as pd\npd.options.display.max_columns = 0\n</code></pre> location_state expiration_date zip_code address_number_start issued_date city location_zip police_district latitude state location_address location_city payment_date longitude account_number site_number ward doing_business_as_name street_type permit_number address legal_name address_number street street_direction geometry source_data_updated ingestion_check_time 0 2024-02-29 00:00:00 60647 0 2023-06-22 00:00:00 CHICAGO IL 2023-06-22 00:00:00 426327 1 1 MINI MOTT BLVD 1829707 0 W LOGAN BLVD MINI MOTT CO. 0 LOGAN W 0101000020E6100000000000000000F87F000000000000F87F 2023-06-24T09:48:47Z 2023-06-25T03:50:03.938039Z 1 2024-02-29 00:00:00 60618 2890 2023-06-09 00:00:00 CHICAGO 14 41.9338 IL 2023-06-09 00:00:00 -87.7154 432506 1 35 LA CELIA LATIN KITCHEN AVE 1829005 2890 N MILWAUKEE AVE LA CELIA LLC 2890 MILWAUKEE N 0101000020E6100000B3EEF24EC8ED55C0DF3844EB85F74440 2023-06-24T09:48:47Z 2023-06-25T03:50:03.938039Z 2 2024-02-29 00:00:00 60610 0 2023-06-15 00:00:00 CHICAGO 1 41.8821 IL 2023-06-15 00:00:00 -87.628 478579 1 42 Cafe Sophie ST 1828623 0 N STATE ST CAFE SOPHIE GOLD COAST LLC 0 STATE N 0101000020E61000007A196F8E30E855C0698D6D16E8F04440 2023-06-24T09:48:47Z 2023-06-25T03:50:03.938039Z 3 2024-02-29 00:00:00 60622 0 2023-06-23 00:00:00 CHICAGO 18 41.9041 IL 2023-06-23 00:00:00 -87.6287 7533 1 26 LA BRUQUENA RESTAURANT &amp; LOUNGE ST 1828620 0 W DIVISION ST LA BRUQUENA RESTAURANT &amp; LOUNGE, INC. 0 DIVISION W 0101000020E61000005EFC06633DE855C096906BEDB7F34440 2023-06-24T09:48:47Z 2023-06-25T03:50:03.938039Z 4 2024-02-29 00:00:00 60657 3328 2023-06-22 00:00:00 CHICAGO 19 41.9424 IL 2023-06-22 00:00:00 -87.6707 482604 2 32 PWU DUMMY ACCOUNT AVE 1827202 3328 N LINCOLN AVE PWU DUMMY ACCOUNT 3328 LINCOLN N 0101000020E6100000B1ABBBF9ECEA55C0A31D5016A0F84440 2023-06-24T09:48:47Z 2023-06-25T03:50:03.938039Z <p>Now that we have a <code>Validator</code> instance, we can use its <code>expect_*</code> methods to start defining expectations.</p> How-to: View available <code>Expectation</code> types <p>Run command <code>validator.list_available_expectation_types()</code> to get a list of <code>Expectation</code> methods available through the <code>Validator</code> instance (as shown below). Note: some expectations aren't implemented for all sources; check here to review which <code>Expectation</code>s are implemented for each backend.</p> <p><code>validator.list_available_expectation_types()</code></p> <pre><code>[\n    'expect_column_distinct_values_to_be_in_set',\n    'expect_column_distinct_values_to_contain_set',\n    'expect_column_distinct_values_to_equal_set',\n    'expect_column_kl_divergence_to_be_less_than',\n    'expect_column_max_to_be_between',\n    'expect_column_mean_to_be_between',\n    'expect_column_median_to_be_between',\n    'expect_column_min_to_be_between',\n    'expect_column_most_common_value_to_be_in_set',\n    'expect_column_pair_values_a_to_be_greater_than_b',\n    'expect_column_pair_values_to_be_equal',\n    'expect_column_pair_values_to_be_in_set',\n    'expect_column_proportion_of_unique_values_to_be_between',\n    'expect_column_quantile_values_to_be_between',\n    'expect_column_stdev_to_be_between',\n    'expect_column_sum_to_be_between',\n    'expect_column_to_exist',\n    'expect_column_unique_value_count_to_be_between',\n    'expect_column_value_lengths_to_be_between',\n    'expect_column_value_lengths_to_equal',\n    'expect_column_value_z_scores_to_be_less_than',\n    'expect_column_values_to_be_between',\n    'expect_column_values_to_be_dateutil_parseable',\n    'expect_column_values_to_be_decreasing',\n    'expect_column_values_to_be_in_set',\n    'expect_column_values_to_be_in_type_list',\n    'expect_column_values_to_be_increasing',\n    'expect_column_values_to_be_json_parseable',\n    'expect_column_values_to_be_null',\n    'expect_column_values_to_be_of_type',\n    'expect_column_values_to_be_unique',\n    'expect_column_values_to_match_json_schema',\n    'expect_column_values_to_match_like_pattern',\n    'expect_column_values_to_match_like_pattern_list',\n    'expect_column_values_to_match_regex',\n    'expect_column_values_to_match_regex_list',\n    'expect_column_values_to_match_strftime_format',\n    'expect_column_values_to_not_be_in_set',\n    'expect_column_values_to_not_be_null',\n    'expect_column_values_to_not_match_like_pattern',\n    'expect_column_values_to_not_match_like_pattern_list',\n    'expect_column_values_to_not_match_regex',\n    'expect_column_values_to_not_match_regex_list',\n    'expect_compound_columns_to_be_unique',\n    'expect_multicolumn_sum_to_equal',\n    'expect_select_column_values_to_be_unique_within_record',\n    'expect_table_column_count_to_be_between',\n    'expect_table_column_count_to_equal',\n    'expect_table_columns_to_match_ordered_list',\n    'expect_table_columns_to_match_set',\n    'expect_table_row_count_to_be_between',\n    'expect_table_row_count_to_equal',\n    'expect_table_row_count_to_equal_other_table'\n]\n</code></pre> <p>To inspect the <code>args</code> and/or <code>kwargs</code> for a method, prefix the call with a question mark to see the docstring (or two question marks to see the docstring and source code).</p> How-to: View args and kwargs for an <code>Expectation</code> <p>To view the docstring (which includes descriptions of args and kwargs), prefix the method with one question mark and run that cell (note: leave off the parentheses). To view the method's docstring and source code, use two question marks.</p> <p><code>?validator.expect_column_values_to_be_of_type</code></p> <pre><code>Signature: validator.expect_column_values_to_be_of_type(*args, **kwargs)\nDocstring:\nExpect a column to contain values of a specified data type.\n\nexpect_column_values_to_be_of_type is a [Column Map Expectation](https://docs.greatexpectations.io/docs/guides/expectations/creating_custom_expectations/how_to_create_custom_column_map_expectations) for typed-column backends, and also for PandasDataset where the column dtype and provided type_ are unambiguous constraints (any dtype except 'object' or dtype of 'object' with     type_ specified as 'object').\n\nFor PandasDataset columns with dtype of 'object' expect_column_values_to_be_of_type will\nindependently check each row's type.\n\nArgs:\n    column (str):  The column name.\n    type\\_ (str):  A string representing the data type that each column should have as entries.\n                    Valid types are defined by the current backend implementation and are\n                    dynamically loaded. For example, valid types for PandasDataset include any\n                    numpy dtype values (such as 'int64') or native python types (such as 'int'),\n                    whereas valid types for a SqlAlchemyDataset include types named by the current\n                    driver such as 'INTEGER' in most SQL dialects and 'TEXT' in dialects such as\n                    postgresql. Valid types for SparkDFDataset include 'StringType', 'BooleanType'\n                    and other pyspark-defined type names. Note that the strings representing these\n                    types are sometimes case-sensitive. For instance, with a Pandas backend\n                    `timestamp` will be unrecognized and fail the expectation, while `Timestamp`\n                    would pass with valid data.\n\nKeyword Args:\n    mostly (None or a float between 0 and 1):   Successful if at least mostly fraction of values match the expectation. For more detail, see [mostly](https://docs.greatexpectations.io/docs/reference/expectations/standard_arguments/#mostly).\n...\n</code></pre> <p>Add <code>Expectations</code> to your <code>Validator</code>-instance's <code>expectation_suite</code> (or edit them if they already exist) by running the relevant <code>validator</code> method. Except for the four standard arguments (<code>result_format</code>, <code>catch_exceptions</code>, <code>meta</code>, and <code>mostly</code>), each <code>Expectation</code>-type can have different arguments (although they're pretty intuitive).</p> <pre><code>validator.expect_table_columns_to_match_set(\n    column_set=[\n        \"account_number\", \"site_number\", \"permit_number\", \"legal_name\", \"doing_business_as_name\",\n        \"issued_date\", \"expiration_date\", \"payment_date\", \"address_number\", \"address_number_start\",\n        \"street_direction\", \"street\", \"street_type\", \"city\", \"state\", \"zip_code\", \"address\",\n        \"ward\", \"police_district\", \"location_state\", \"location_zip\", \"location_address\",\n        \"location_city\", \"longitude\", \"latitude\", \"geometry\", \"source_data_updated\",\n        \"ingestion_check_time\"\n    ],\n    exact_match=True\n)\nvalidator.expect_column_distinct_values_to_be_in_set(column=\"state\", value_set=[\"IL\"])\nvalidator.expect_column_distinct_values_to_be_in_set(column=\"city\", value_set=[\"CHICAGO\"])\n\nvalidator.expect_column_values_to_be_null(column=\"location_address\")\nvalidator.expect_column_values_to_be_null(column=\"location_city\")\nvalidator.expect_column_values_to_be_null(column=\"location_state\")\nvalidator.expect_column_values_to_be_null(column=\"location_zip\")\n\nvalidator.expect_column_values_to_not_be_null(column=\"issued_date\")\nvalidator.expect_column_values_to_not_be_null(column=\"expiration_date\")\nvalidator.expect_column_values_to_not_be_null(column=\"legal_name\")\nvalidator.expect_column_values_to_not_be_null(column=\"doing_business_as_name\")\nvalidator.expect_column_values_to_not_be_null(column=\"source_data_updated\")\nvalidator.expect_column_values_to_not_be_null(column=\"ingestion_check_time\")\nvalidator.expect_column_values_to_not_be_null(column=\"permit_number\")\nvalidator.expect_column_values_to_not_be_null(column=\"account_number\")\nvalidator.expect_column_values_to_not_be_null(column=\"zip_code\", mostly=0.99)\n\nvalidator.expect_column_values_to_be_of_type(column=\"issued_date\", type_=\"TIMESTAMP\")\nvalidator.expect_column_values_to_be_of_type(column=\"expiration_date\", type_=\"TIMESTAMP\")\nvalidator.expect_column_values_to_be_of_type(column=\"payment_date\", type_=\"TIMESTAMP\")\n\nvalidator.expect_column_values_to_be_unique(column=\"permit_number\")\nvalidator.expect_column_values_to_be_unique(column=\"account_number\")\nvalidator.expect_column_values_to_be_unique(column=\"site_number\")\n\nvalidator.expect_column_unique_value_count_to_be_between(\n    column=\"ward\",\n    min_value=1,\n    max_value=50,\n    meta={\n        \"notes\": {\n            \"format\": \"markdown\",\n            \"content\": (\n                \"Some markdown-formatted comment about this expectation fo be included in the \"\n                + \"Data Docs entry for this validation. **Markdown** `Supported`, $\\latex$ too.\"\n            ),\n        }\n    }\n)\n...\n</code></pre> Standard argument example: <code>result_format</code> <p>The valid <code>result_format</code> types are:</p> <ul> <li>\"BASIC\" (default)</li> <li>\"BOOLEAN_ONLY\"</li> <li>\"SUMMARY\"</li> <li>\"COMPLETE\"</li> </ul> <p>\"BASIC\"</p> <p><pre><code>validator.expect_column_value_lengths_to_be_between(\n    column=\"doing_business_as_name\",\n    min_value=1,\n    max_value=127,\n    result_format=\"BASIC\"\n)\n</code></pre> outputs <pre><code>{\n    \"meta\": {},\n    \"success\": true,\n    \"result\": {\n        \"element_count\": 58,\n        \"unexpected_count\": 0,\n        \"unexpected_percent\": 0.0,\n        \"partial_unexpected_list\": [],\n        \"missing_count\": 0,\n        \"missing_percent\": 0.0,\n        \"unexpected_percent_total\": 0.0,\n        \"unexpected_percent_nonmissing\": 0.0\n    },\n    \"exception_info\": {\n        \"raised_exception\": false,\n        \"exception_traceback\": null,\n        \"exception_message\": null\n    }\n}\n</code></pre></p> <p>\"BOOLEAN_ONLY\"</p> <p><pre><code>validator.expect_column_value_lengths_to_be_between(\n    column=\"doing_business_as_name\",\n    min_value=1,\n    max_value=127,\n    result_format=\"BOOLEAN_ONLY\"\n)\n</code></pre> outputs <pre><code>{\n    \"meta\": {},\n    \"success\": true,\n    \"result\": {},\n    \"exception_info\": {\n        \"raised_exception\": false,\n        \"exception_traceback\": null,\n        \"exception_message\": null\n    }\n}\n</code></pre></p> <p>\"SUMMARY\"</p> <p><pre><code>validator.expect_column_value_lengths_to_be_between(\n    column=\"doing_business_as_name\",\n    min_value=1,\n    max_value=127,\n    result_format=\"SUMMARY\"\n)\n</code></pre> outputs <pre><code>{\n    \"meta\": {},\n    \"success\": true,\n    \"result\": {\n        \"element_count\": 58,\n        \"unexpected_count\": 0,\n        \"unexpected_percent\": 0.0,\n        \"partial_unexpected_list\": [],\n        \"missing_count\": 0,\n        \"missing_percent\": 0.0,\n        \"unexpected_percent_total\": 0.0,\n        \"unexpected_percent_nonmissing\": 0.0,\n        \"partial_unexpected_counts\": []\n    },\n    \"exception_info\": {\n        \"raised_exception\": false,\n        \"exception_traceback\": null,\n        \"exception_message\": null\n    }\n}\n</code></pre></p> <p>\"COMPLETE\"</p> <p><pre><code>validator.expect_column_value_lengths_to_be_between(\n    column=\"doing_business_as_name\",\n    min_value=1,\n    max_value=127,\n    result_format=\"COMPLETE\"\n)\n</code></pre> outputs <pre><code>{\n    \"meta\": {},\n    \"success\": true,\n    \"result\": {\n        \"element_count\": 58,\n        \"unexpected_count\": 0,\n        \"unexpected_percent\": 0.0,\n        \"partial_unexpected_list\": [],\n        \"missing_count\": 0,\n        \"missing_percent\": 0.0,\n        \"unexpected_percent_total\": 0.0,\n        \"unexpected_percent_nonmissing\": 0.0,\n        \"partial_unexpected_counts\": [],\n        \"unexpected_list\": [],\n        \"unexpected_index_query\": \"SELECT doing_business_as_name \\nFROM data_raw.temp_chicago_sidewalk_cafe_permits \\nWHERE doing_business_as_name IS NOT NULL AND NOT (length(doing_business_as_name) &gt;= 1 AND length(doing_business_as_name) &lt;= 127);\"\n    },\n    \"exception_info\": {\n        \"raised_exception\": false,\n        \"exception_traceback\": null,\n        \"exception_message\": null\n    }\n}\n</code></pre></p> <p>When you're content with your suite of <code>Expectations</code>, save them to your <code>DataContext</code>.</p> <pre><code>validator.save_expectation_suite(discard_failed_expectations=False)\n</code></pre> <p>Now you can add (or update) a <code>Checkpoint</code> to evaluate that suite of <code>Expectations</code>. This step can take a while (depending on the size of the table and the number of <code>Expectations</code> being evaluated).</p> <pre><code>checkpoint = context.add_or_update_checkpoint(\n    name=f\"{data_asset_name}_checkpoint\",\n    validator=validator,\n)\ncheckpoint_result = checkpoint.run()\n</code></pre> <p>After running the <code>Checkpoint</code>, report the results (by building Data Docs).</p> <pre><code>context.build_data_docs()\n</code></pre>"},{"location":"dev_guide/validation/new_workflow/#reviewing_data_docs","title":"Reviewing Data Docs","text":"<p>Navigate to <code>/uncommitted/data_docs/local_site</code> in the File Browser and open the file named <code>index.html</code>.</p> How-to: Navigate Data Docs <p>If you can't click anything and your Data Docs interface looks like this, click \"Trust HTML\" in the upper left corner.</p> <p></p> <p>If you see checks of batches where the validation status indicates failure (i.e. a  mark in the Status column), then at least one <code>Expectation</code> was not met. Click into a report to see the failed expectations.</p> <p></p> <p>Click Failed Only in the left-hand Actions: Validation Filter box to only see failed <code>Expectations</code>.</p> <p></p> <p>We see that our uniqueness expectation for <code>account_number</code> and <code>site_number</code> wasn't met and isn't valid. Let's remove those expectations.</p>"},{"location":"dev_guide/validation/new_workflow/#removing_an_expectation","title":"Removing an <code>Expectation</code>","text":"<p>While reviewing expectations, you may find an expectation you want to just remove. You can delete such expectations via the <code>validator</code> object's <code>.remove_expectation()</code> method, but you have to pass in the expectation that is to be removed from the suite. You can access the expectation by accessing the <code>.expectation_config</code> attr of the <code>ExpectationValidationResult</code>.</p> <pre><code>expectation_validation_result = validator.expect_column_values_to_not_be_null(column=\"account_number\")\n\nprint(f\"Expectation configs in suite pre-removal: {len(validator.expectation_suite.expectations)}\")\nvalidator.remove_expectation(\n    expectation_configuration=expectation_validation_result.expectation_config\n)\nvalidator.remove_expectation(\n    validator.expect_column_values_to_be_unique(column=\"site_number\").expectation_config\n)\nprint(f\"Expectation configs in suite post-removal: {len(validator.expectation_suite.expectations)}\")\n</code></pre> <p>After removing those, save the suite to our context.</p> <pre><code>validator.save_expectation_suite(discard_failed_expectations=False)\n</code></pre>"},{"location":"dev_guide/validation/new_workflow/#rerunning_checkpoints","title":"Rerunning Checkpoints","text":"<p>Update the <code>Checkpoint</code> with our modified <code>validator</code> and rerun.</p> <pre><code>checkpoint = context.add_or_update_checkpoint(\n    name=f\"{data_asset_name}_checkpoint\",\n    validator=validator,\n)\ncheckpoint_result = checkpoint.run()\n</code></pre> <p>After running the <code>Checkpoint</code>, rebuild and review Data Docs.</p> <pre><code>context.build_data_docs()\n</code></pre> <p>If all <code>Expectations</code> are being met for all batches, the Data Docs index should look like this (with green check mark circles  in the Status column)</p> <p></p>"},{"location":"dev_guide/validation/new_workflow/#summary","title":"Summary","text":"<p>We've defined a suite of <code>Expectations</code> for batches of data as well as configured a <code>Checkpoint</code> to handle validating our data. Now we can run that <code>Checkpoint</code> when we update this <code>DataAsset</code>.</p>"},{"location":"overview/pipeline_patterns/","title":"Standard Data Update Pipeline","text":"<p>At a high level, ADWH raw data update pipelines follow a basic pattern. If the data source has new data, collect that data and update the corresponding table(s) in the local warehouse.  </p> <pre><code>flowchart TB\n    start([Start Data Update DAG])\n    start --&gt; check_source\n    subgraph Freshness Check\n        direction TB\n        check_source[Check Souce]\n        check_source --&gt; is_update_needed{Is Warehouse\\nData Stale?}\n    end\n    is_update_needed -- No --&gt; record_results\n    is_update_needed -- Yes --&gt; download_data\n    subgraph Update Raw Data in Warehouse\n        direction TB\n        download_data[Collect New Data] --&gt; validate_temp{Data meets\\n Expectations}\n        validate_temp -- Yes --&gt; insert_records_not_in_persistant[Insert New and\\nUpdated Records to\\nPersistant Table]\n\n    end\n    validate_temp -- No --&gt; record_results\n    insert_records_not_in_persistant --&gt; record_results\n    record_results[Record Metadata] --&gt; end_dag([End Data Update DAG])</code></pre>"},{"location":"setup/","title":"Setup Overview","text":"<p>This platform is easy to setup while still being secure enough to open source the code without exposing keys or credentials.</p>"},{"location":"setup/#system_requirements","title":"System Requirements","text":"<ul> <li>Docker with <code>Compose</code> v2.0 or higher.</li> <li>a basic system install of python (for running the script to set up credentials).</li> <li>Optional (but recommended): GNU make (for running recipes that set up infrastructure).</li> </ul> <p>Note: keep an eye on system storage. This platform automates collection and does not automatically purge old data files.</p>"},{"location":"setup/#setup_steps","title":"Setup Steps","text":"<p>Work through these steps to set up all ADWH credentials and services. It should take around 10 minutes.</p> <ol> <li>Set up your credentials and build the images</li> <li>Configure database connections for Superset</li> <li>Configure database connections for pgAdmin4</li> <li>Configure database connections and ingestions for OpenMetadata</li> </ol>"},{"location":"setup/dbt/","title":"Dbt","text":""},{"location":"setup/dbt/#specifying_installing_and_updating_dbt_packages","title":"Specifying, installing, and updating dbt packages","text":"<p>Create a file named <code>packages.yml</code> in your dbt project directory and specify any packages you want to use in your project in the format shown below (or as shown in the documentation)</p> <pre><code>packages:\n  - package: dbt-labs/dbt_utils\n    version: 0.9.2\n</code></pre> <p>Then, after specifying packages and versions to use, run this command to install packages.</p> <pre><code>user@host:.../your_local_repo$ make update_dbt_packages\n01:33:04  Running with dbt=1.3.0\n01:33:05  Installing dbt-labs/dbt_utils\n01:33:05    Installed from version 0.9.2\n01:33:05    Up to date!\n</code></pre>"},{"location":"setup/getting_started/","title":"System Setup","text":"<p>Preprequisites: To use this system, Docker is the only absolutely necessary prerequisite.</p> <p>Having <code>GNU make</code> on your host system will enable you to use included <code>makefile</code> recipes to streamline setup and common operations, but if don't have and can't install it on your system, you can just manually run the commands from recipes in the <code>makefile</code> in the project's top-level directory.</p>"},{"location":"setup/getting_started/#setting_up_credentials","title":"Setting up credentials","text":"<p>After cloning this project, <code>cd</code> into your local repo, run this <code>make</code> command and enter appropriate responses to the prompts. You may want to have a password generator open.</p> <pre><code>make make_credentials\n</code></pre> <p>The program will validate the values you enter, assemble them into some compound values (mostly connection strings), and output these configs into the dot-env files (<code>.env</code>, <code>.env.dwh</code>, and <code>.env.superset</code>) in the top-level repo directory. Review these files and make any changes before you initialize the system (i.e., when these username and password pairs are used to create roles in databases or Airflow starts using the Fernet key to encrypt passwords in connection strings).</p>"},{"location":"setup/getting_started/#requesting_a_census_api_key","title":"Requesting a Census API Key","text":"<p>To get a Census API key:</p> <ol> <li>Go to https://www.census.gov/data/developers.html and click the Request a KEY button.</li> <li>Enter your email address and an \"Organization Name\" to associate to the API key.</li> <li>Copy your new API key from your email and paste it into your <code>.env</code> file for the <code>CENSUS_API_KEY</code> variable.</li> </ol>"},{"location":"setup/getting_started/#optional_mapbox_api_token_for_geospatial_superset_maps","title":"[Optional] Mapbox API Token for Geospatial Superset Maps","text":"<p>To include a basemap underneath geospatial visualizations in Superset, you'll need to:</p> <ol> <li>Create a free Mapbox account,</li> <li>Create a new API access token on your Mapbox account page, and</li> <li> <p>Open your <code>.env.superset</code> dot-env file and paste that API access token in for the <code>MAPBOX_API_KEY</code> environment variable so that it looks like the example below.</p> <p><code>MAPBOX_API_KEY=pk.&lt;the rest of the API token you created&gt;</code></p> </li> </ol> <p>You can still make geospatial Superset charts without an API key, but your geospatial charts won't have a basemap (like the left example).</p> <p> </p>"},{"location":"setup/getting_started/#initializing_the_system","title":"Initializing the system","text":"<p>Before you initialize</p> <p>If you want to change any credentials, do this before you start up the system. It's easiest to do this by renaming (or removing) all <code>.env</code> files, rerunning <code>make make_credentials</code>, and entering the desired credentials (and remember to remove your prior <code>.env</code> files if you kept them earlier). You can change many of the credentials after the system is up and running, but changing credentials for existing databases is not currently supported.</p> <p>On the first startup of the system (and after setting your credentials), run the commands below to 1. build the platform's docker images, and initialize the airflow metadata database, 2. start up the system, and after console outputs stabilize (i.e., when services are running) 3. create the <code>metadata</code> and <code>data_raw</code> schemas and the <code>metadata.table_metadata</code> table in your data warehouse database.</p> <pre><code>make initialize_system\ndocker compose up\n</code></pre> <p>and in another terminal (after <code>cd</code>ing back into the project dir)</p> <pre><code>make create_warehouse_infra\n</code></pre> <p>Now the core of the system is up and running, and you can proceed to the next section to set up database connections that will enable you to explore the data and databases through the pgAdmin4 and Superset interfaces.</p>"},{"location":"setup/openMetadata/","title":"Setting up OpenMetadata","text":""},{"location":"setup/openMetadata/#logging_into_the_openmetadata_ui","title":"Logging into the OpenMetadata UI","text":"<p>To access the OpenMetadata UI, go to http://localhost:8585 and log in using credentials defined in the initial setup step.</p> <ul> <li> <p>Email: This will consist of two environment variables from <code>.env.om_server</code> in the form below. The default email is <code>admin@open-metadata.org</code>.</p> <p><code>AUTHORIZER_ADMIN_PRINCIPALS@AUTHORIZER_PRINCIPAL_DOMAIN</code></p> </li> <li> <p>Password: the default password will be <code>admin</code>. You should probably change that password on your first login.</p> </li> </ul> <p></p> <p>Note</p> <p>If you're hosting the ADWH system on another machine, replace <code>localhost</code> with the domain name or IP addess of that remote machine.</p>"},{"location":"setup/openMetadata/#define_a_connection_to_the_data_warehouse_database","title":"Define a Connection to the Data Warehouse Database","text":"<ol> <li> <p>Go to Settings &gt; Services &gt; Databases and Add a New Service</p> <p> </p> </li> <li> <p>Make a Postgres Database Service</p> <p></p> <p>2.a. Enter a name and a brief description of the database.</p> <p></p> <p>2.b. Enter credentials for the DWH database role you want the service to use along with the other connection info. After entering credentials and other info, Test your Connection.</p> <p> </p> <p>If all connection checks pass, click OK and Save the connection.</p> </li> </ol> <p>If everything was successful, you should now see your <code>ADWH Data Warehouse</code> Database Service.</p> <p></p>"},{"location":"setup/openMetadata/#configure_metadata_ingestion","title":"Configure Metadata Ingestion","text":"<p>Contuing from the last image, go to the page for the <code>ADWH Data Warehouse</code> Database Service. You will see the different postgres databases in the <code>dwh_db</code> postgres instance.</p> <p>To scan those databases for Data Assets to catalog, you have to configure an Ingestion.</p> <ol> <li> <p>Click Ingestions &gt; Add Ingestion &gt; Add Metadata Ingestion</p> <p></p> <p></p> </li> <li> <p>Specify the assets to include in this Metadata Ingestion configuration</p> <p>Enter regex patterns to specify which database(s), schema(s), and table(s) should be included.</p> <p></p> <p></p> </li> <li> <p>Set the schedule for running this Ingestion then Add &amp; Deploy it</p> <p> </p> </li> <li> <p>Trigger an initial Run to immediately run the Metadata Ingestion</p> <p></p> </li> </ol> <p>Now you should have a catalog of metadata for all Data Assets in the main DWH schemas.</p>"},{"location":"setup/openMetadata/#changing_your_password","title":"Changing your password","text":"<ol> <li> <p>From the user profile dropdown in the upper right corner, click your username to addess your user page</p> <p></p> </li> <li> <p>Change your password</p> <p></p> <p></p> </li> </ol>"},{"location":"setup/pgAdmin4/","title":"Setting up pgAdmin4","text":"<p>The pgAdmin4 UI makes it very easy to explore your data, inspect database internals, and make manual changes while developing features, but before you can make use of this excellent interface, you have to log in and set up database connection(s).</p>"},{"location":"setup/pgAdmin4/#logging_into_the_pgadmin4_database_administration_ui","title":"Logging into the pgAdmin4 database administration UI","text":"<p>To access the pgAdmin4 database administration UI, go to http://localhost:5678 and log in using credentials defined in the initial setup step.</p> <ul> <li>Email Address / Username: use the <code>PGADMIN_DEFAULT_EMAIL</code> value in your <code>.env</code> file, and</li> <li>Password: use the <code>PGADMIN_DEFAULT_PASSWORD</code> value in your <code>.env</code> file.</li> </ul> <p></p>"},{"location":"setup/pgAdmin4/#setting_up_database_connections_in_pgadmin4","title":"Setting up database connections in pgAdmin4","text":"<p>This platform uses two separate databases: one as a backend for Airflow, and the other as the data warehouse database.</p> <p> </p> <p>Before you can use pgAdmin4's interface to explore databases in either of these database servers, you have to set up connections to those database servers.</p> <p></p> <p>The values needed to set up these connections will mainly come from a dot-env file (<code>.env</code> or <code>.env.dwh</code>) or from service names in the <code>docker-compose.yml</code> file.</p>"},{"location":"setup/pgAdmin4/#airflow_metadata_database","title":"Airflow Metadata Database","text":"<p>To create a new connection, start by clicking the \"Add New Server\" button (you might have to click the \"Servers\" line in the lefthand tray first).</p>"},{"location":"setup/pgAdmin4/#register_server_general_tab","title":"Register Server: General tab","text":"<ul> <li>Name: <code>airflow_metadata_db</code><ul> <li>This can be whatever you want, it's just the label that pgAdmin4 will use for the connection.</li> </ul> </li> </ul> <p>Don't worry about the Server group field, the default is fine.</p> <p></p>"},{"location":"setup/pgAdmin4/#register_server_connection_tab","title":"Register Server: Connection tab","text":"<ul> <li>Host name/address: <code>airflow_db</code><ul> <li>This is defined here in the <code>docker-compose.yml</code> file</li> </ul> </li> <li>Port: 5432<ul> <li>This is the database's port number inside the container, as defined to the right of the colon here.</li> </ul> </li> <li>Username: the <code>POSTGRES_USER</code> value in your <code>.env</code> file.</li> <li>Password: the <code>POSTGRES_PASSWORD</code> value in your <code>.env</code> file.</li> </ul> <p>The defaults for Maintenance database, Role, Service, and the two toggles (of <code>postgres</code>, blank, blank, off, and off, respectively) are all fine.</p> <p></p> <p>Then press Save to finalize the connection.</p>"},{"location":"setup/pgAdmin4/#data_warehouse_database","title":"Data Warehouse Database","text":"<p>Repeat the process to connect to the data warehouse database.</p>"},{"location":"setup/pgAdmin4/#register_server_general_tab_1","title":"Register Server: General tab","text":"<ul> <li>Name: <code>data_warehouse_db</code><ul> <li>As before, this can be whatever you want</li> </ul> </li> </ul>"},{"location":"setup/pgAdmin4/#register_server_connection_tab_1","title":"Register Server: Connection tab","text":"<ul> <li>Host name/address: <code>dwh_db</code><ul> <li>This is defined here in the <code>docker-compose.yml</code> file</li> </ul> </li> <li>Port: 5432<ul> <li>This is the database's port number inside the container, as defined to the right of the colon here</li> </ul> </li> <li>Username: the <code>DWH_POSTGRES_USER</code> value in your <code>.env</code> file</li> <li>Password: the <code>DWH_POSTGRES_PASSWORD</code> value in your <code>.env</code> file</li> </ul> <p>The defaults for Maintenance database, Role, Service, and the two toggles (of <code>postgres</code>, blank, blank, off, and off, respectively) remain all fine.</p> <p></p> <p>Press Save to finalize the connection.</p> <p>Now pgAdmin4 is configured! Click around, explore the dropdowns, right-click things and see what you see. This is an excellent way to learn about the internals of postgres!</p> <p></p>"},{"location":"setup/superset_setup/","title":"Setting up Superset","text":"<p>Log in to superset at http://localhost:8088 using the <code>ADMIN_USERNAME</code> and <code>ADMIN_PASSWORD</code> from your <code>.env.superset</code> file.</p>"},{"location":"setup/superset_setup/#connecting_to_the_data_warehouse_database","title":"Connecting to the Data Warehouse Database","text":"<p>Before you can develop charts and dashboards in Apache Superset, you have to create a connection to your data warehouse database.</p> <p>Click the  (in the upper right corner) and then select the Connect database option from the  Data menu. </p> <p></p> <p>Select the PostgreSQL option and then enter the following credentials:</p> <ul> <li>HOST: enter dwh_db (must match the name of the DB service in the <code>docker-compose.yml</code>)</li> <li>PORT: 5432</li> <li>DATABASE NAME: enter the value of the DWH_POSTGRES_DB env var from your <code>.env</code> file</li> <li>USERNAME: enter the value of the DWH_POSTGRES_USER env var from your <code>.env</code> file</li> <li>PASSWORD: enter the value of the DWH_POSTGRES_PASSWORD env var from your <code>.env</code> file</li> <li>DISPLAY NAME: Anything descriptive (recommended: dwh_db)</li> </ul> <p> </p> <p>If credentials and configs were entered correctly (and the database container is running), superset will successfully connect to the database and you can save the connection by clicking FINISH.</p> <p></p>"},{"location":"user_guide/","title":"User's Guide","text":"<p>This system comes with a number of already-built ELT data pipelines that you can run to stock your data warehouse. This User's Guide shows:</p> <ul> <li>how to start up the system</li> <li>how to unpause and run a pipeline</li> <li>how to create charts and dashboards from the data in your warehouse.</li> </ul> <p>To add a pipeline for new data set, see the Developer's Guide.</p>"},{"location":"user_guide/accessing_resources/","title":"Accessing Systems","text":"<p>After systems have started up, you can access:</p>"},{"location":"user_guide/accessing_resources/#the_pgadmin4_database_administration_ui","title":"The pgAdmin4 database administration UI","text":"<p>Access the pgAdmin4 database administration UI at http://localhost:5678 * Log in using the <code>PGADMIN_DEFAULT_EMAIL</code> and <code>PGADMIN_DEFAULT_PASSWORD</code> credentials from your <code>.env</code> file.</p> <p></p>"},{"location":"user_guide/accessing_resources/#the_airflow_web_ui","title":"The Airflow web UI","text":"<p>Access the Airflow webserver user interface at http://localhost:8080</p> <ul> <li>Log in using the <code>_AIRFLOW_WWW_USER_USERNAME</code> and <code>_AIRFLOW_WWW_USER_PASSWORD</code> credentials from your <code>.env</code> file.</li> </ul> <p></p>"},{"location":"user_guide/accessing_resources/#the_dbt_data_documentation_and_discovery_ui","title":"The dbt Data Documentation and Discovery UI","text":"<p>Run the command below to generate and serve documentation for the data transformations executed by dbt. After the doc server has started up, go to http://localhost:18080 to explore the documentation UI.</p> <pre><code>user@host:.../your_local_repo$ make serve_dbt_docs\n</code></pre> <p></p> <p></p>"},{"location":"user_guide/adding_a_socrata_pipeline/","title":"Adding a socrata pipeline","text":""},{"location":"user_guide/adding_a_socrata_pipeline/#1_adding_a_new_pipeline_for_a_table_hosted_by_socrata","title":"1. Adding a new Pipeline for a table hosted by Socrata","text":"<p>After the system is set up, you can easily add a Socrata data set to the warehouse by</p> <p>Define the <code>SocrataTable</code> in <code>/airflow/dags/sources/tables.py</code>:</p> <p>Look up the table's documentation page on the web and get the <code>table_id</code> from the URL (it will be nine characters long, all lowercase and with a hyphen in the middle).</p> <pre><code>SAMPLE_DATA_SET = SocrataTable(\n    table_id=\"wvhk-k5uv\",             # (1)\n    table_name=\"sample_data_set\",   # (2)\n    schedule=\"0 6 4 * *\",             # (3)\n    )\n</code></pre> <ol> <li>A Socrata table's <code>table_id</code> will always be 9 characters long and consist of two blocks of 4 characters (numbers or lowercase letters) separated by a hyphen. You can find the <code>table_id</code> in the data documentation URL or export link for the data set.</li> <li>Ideally the name of the <code>SocrataTable</code> instance should be the uppercased <code>table_name</code> (which should be lowercase).</li> <li>This sets the update frequency. If you aren't familiar with the <code>crontab</code> format, use cron expressions.</li> </ol> <p></p>"},{"location":"user_guide/adding_a_socrata_pipeline/#2_create_a_dag_for_the_data_set","title":"2. Create a DAG for the data set","text":"<p>Copy this code into a new file in <code>/airflow/dags/</code> and edit the 4 annotated lines for the new data set:</p> <pre><code>import datetime as dt\nimport logging\n\nfrom airflow.decorators import dag\n\nfrom tasks.socrata_tasks import update_socrata_table\nfrom sources.tables import COOK_COUNTY_PARCEL_SALES as SOCRATA_TABLE   # (1)\n\ntask_logger = logging.getLogger(\"airflow.task\")\n\n\n@dag(\n    schedule=SOCRATA_TABLE.schedule,\n    start_date=dt.datetime(2022, 11, 1),\n    catchup=False,\n    tags=[\"cook_county\", \"parcels\", \"fact_table\", \"data_raw\"],        # (2)\n)\ndef update_data_raw_sample_data_set():                                # (3)\n    update_1 = update_socrata_table(\n        socrata_table=SOCRATA_TABLE,\n        conn_id=\"dwh_db_conn\",\n        task_logger=task_logger,\n    )\n    update_1\nupdate_data_raw_sample_data_set()                                     # (4)\n</code></pre> <ol> <li>Replace <code>COOK_COUNTY_PARCEL_SALES</code> with the name of the <code>SocrataTable</code> instance variable from <code>tables.py</code>.</li> <li>Change the tags to reflect this data set.</li> <li>Change the name of this DAG's function name to reflect this data set.</li> <li>Call that DAG function.</li> </ol>"},{"location":"user_guide/running_a_pipeline/","title":"Running a Pipeline","text":"<p>To run a data ELT pipeline:</p> <ol> <li> <p>Unpause the DAG for that pipeline.</p> <p>Note: Unpausing a scheduled DAG for the first time will trigger a run. To manually trigger a run, click the  icon in the Actions column or near the upper righthand corner in subsequent DAG views.</p> <p></p> </li> <li> <p>Click that DAG's name to enter its grid view.</p> <p></p> </li> <li> <p>Click Graph to enter the DAG's graph view.</p> <p></p> </li> <li> <p>If all tasks end successfully, the Status indicator will switch from running (light green) to success (dark green).</p> <p></p> </li> </ol> <p>Now that you have some data in your warehouse, you can explore that data in Superset.</p>"},{"location":"user_guide/running_a_pipeline/#troubleshooting","title":"Troubleshooting","text":"<p>Occassionally a task will fail. To investigate</p> <ol> <li> <p>Click on the failed DAG run (will be red in the bar chart on the left) and click the Graph button in that DAG run's details.</p> <p></p> </li> <li> <p>Identify and click the task that failed (it will have a red outline and the tooltip will show Status: failed).</p> <p></p> </li> <li> <p>Click the Log button in the failed task instance's detail pop-up.</p> <p></p> </li> <li> <p>Review the output in the logs to see the error output. You'll probably have to debug the issue or raise the issue upstream before the DAG will run successfully.</p> <p> </p> </li> </ol>"},{"location":"user_guide/running_a_pipeline/#resources","title":"Resources","text":"<p>See the Airflow UI documentation for more information about available views and interfaces.</p>"},{"location":"user_guide/system_startup/","title":"ADWH System Startup","text":"<p>In a terminal, navigate to the project's root directory (which contains the <code>docker-compose.yml</code> file and dot-env files) and start up the system.</p> <pre><code>docker compose up\n</code></pre> <p>System services will begin spinning up and you will see a significant amount of console output. All services should be up and running after 20 to 40 seconds and the output will slow to a crawl.</p> <p>Now you can access Airflow and Superset (and any other services with a web UI) in a browser.</p>"},{"location":"user_guide/system_startup/#access_airflow_at_httplocalhost8080","title":"Access Airflow at http://localhost:8080","text":"<ul> <li>Username: _AIRFLOW_WWW_USER_USERNAME value (from your <code>.env</code> file)</li> <li>Password: _AIRFLOW_WWW_USER_PASSWORD value (from your <code>.env</code> file)</li> </ul>"},{"location":"user_guide/system_startup/#access_superset_at_httplocalhost8088","title":"Access Superset at http://localhost:8088","text":"<ul> <li>Username: ADMIN_USERNAME value (from your <code>.env.superset</code> file)</li> <li>Password: ADMIN_PASSWORD value (from your <code>.env.superset</code> file)</li> </ul>"},{"location":"user_guide/visualization/","title":"Superset","text":"<p>This system uses Superset as a platform for EDA, data visualization, BI, dashboarding, and report generation. Per the Apache Superset Introduction page</p> <p>Apache Superset is a modern, enterprise-ready business intelligence web application. It is fast, lightweight, intuitive, and loaded with options that make it easy for users of all skill sets to explore and visualize their data, from simple pie charts to highly detailed deck.gl geospatial charts.</p> <p>While the system is running, you can access the Superset interface at http://localhost:8088.  </p>"},{"location":"user_guide/visualization/#sample_charts","title":"Sample Charts","text":""},{"location":"user_guide/visualization/#dashboarding","title":"Dashboarding","text":""},{"location":"user_guide/visualization/#charts","title":"Charts","text":""},{"location":"user_guide/visualization/visualization/","title":"Making Visualizations","text":"<p>Create (via the Superset SQL Lab) or select a data set and select a chart type to visualize that data set.</p> <p></p> <p></p> <p>Select dimensions to group data by and metrics to visualize for those groupings.</p> <p>Metrics can be basic aggregations like <code>MIN</code>, <code>MAX</code>, <code>COUNT</code>, <code>COUNT_DISTINCT</code>, <code>AVG</code>, or <code>SUM</code>, or you can define custom aggregations from one or more column.</p> <p></p> <p>Filter data to a meaningful subset.</p> <p></p> <p>Format titles, labels, tooltips, timespan slicing, etc.</p> <p></p> <p>And when the chart is finished, save the visualization (via the save button in the upper right corner). Now the chart is ready to to add to a dashboard.</p> <p></p>"},{"location":"user_guide/visualization/visualization/#resources","title":"Resources","text":"<p>Explore this tutorial for more on visualizing data in Superset.  Preset also provides walkthroughs for many other chart types available in Superset. </p>"}]}